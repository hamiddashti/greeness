{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import my_funs \n",
    "import xarray as xr\n",
    "import cProfile\n",
    "import zarr\n",
    "import matplotlib.pylab as plt\n",
    "# dir = \"/home/hamid/NASA_ABoVE/greeness/\"\n",
    "dir = \"/data/home/hamiddashti/hamid/nasa_above/greeness/\"\n",
    "# out_dir = \"/home/hamid/NASA_ABoVE/greeness/working/\"\n",
    "out_dir = \"/data/home/hamiddashti/hamid/nasa_above/greeness/working\"\n",
    "import folium\n",
    "from folium.plugins import MousePosition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/hamiddashti/miniconda3/envs/geospatial/lib/python3.9/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40155 instead\n",
      "  warnings.warn(\n",
      "2022-08-29 15:31:32,847 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2uz1nczy', purging\n",
      "2022-08-29 15:31:32,847 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t0wr6b17', purging\n",
      "2022-08-29 15:31:32,848 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eigk9ekp', purging\n",
      "2022-08-29 15:31:32,848 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3zz3ftbg', purging\n",
      "2022-08-29 15:31:32,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h1trn73c', purging\n",
      "2022-08-29 15:31:32,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6aieimx8', purging\n",
      "2022-08-29 15:31:32,850 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-trkr9jbg', purging\n",
      "2022-08-29 15:31:32,854 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fve808lz', purging\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lai = xr.open_zarr(dir+'data/raw_data/noaa_cdr/lai_fapar/noaa_lai_fapar_clipped_raw.zarr')\n",
    "lai = ds_lai[\"LAI\"]\n",
    "fapar= ds_lai[\"FAPAR\"]\n",
    "qa_lai = ds_lai[\"QA\"]\n",
    "ds_ndvi = xr.open_zarr(dir+'data/raw_data/noaa_cdr/ndvi/noaa_ndvi_clipped_raw.zarr')\n",
    "ndvi = ds_ndvi[\"NDVI\"]\n",
    "qa_ndvi = ds_ndvi[\"QA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the quality control\n",
    "we only apply the quality control to the LAI and the NDVI. Then we will select spectral bands (red, NIR) where we have good NDVI and calculate the EVI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert QAs to binary\n",
    "qa_lai_binary = my_funs.dec2bin(qa_lai,9)\n",
    "qa_ndvi_binary = my_funs.dec2bin(qa_ndvi,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply mask based on binary QAs aquired from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_lai_binary = qa_lai_binary.astype(str)\n",
    "qa_lai_mask = my_funs.avhrr_mask(qa_lai_binary,dask=\"allowed\",var=\"LAI\")\n",
    "# lai_masked = lai.where(qa_mask)\n",
    "fapar_maked = fapar.where(qa_lai_mask)\n",
    "\n",
    "qa_ndvi_binary = qa_ndvi_binary.astype(str)\n",
    "qa_mask_ndvi =my_funs.avhrr_mask(qa_ndvi_binary,dask=\"allowed\",var=\"NDVI\")\n",
    "ndvi_masked = ndvi.where(qa_mask_ndvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ProgressBar():\n",
    "#     lai_filtered = lai_masked.compute()\n",
    "# with ProgressBar():\n",
    "#     ndvi_filtered = ndvi_masked.compute()\n",
    "\n",
    "# In case staritng the dask distributed client the above progressbar does not work and we should use the following commands\n",
    "fapar_maked_ds = fapar_maked.to_dataset()\n",
    "m = client.compute(fapar_maked_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb991c958ad4099baa44850f34147c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/home/hamiddashti/hamid/nasa_above/greeness/codes/src/noaa_processing_v1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2257484541542d5541227d/data/home/hamiddashti/hamid/nasa_above/greeness/codes/src/noaa_processing_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fapar_maked_ds\u001b[39m.\u001b[39;49mto_zarr(out_dir\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/processed_data/noaa_nc/lai_fapar/filtered/fapar_filtered.nc\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/xarray/core/dataset.py:2060\u001b[0m, in \u001b[0;36mDataset.to_zarr\u001b[0;34m(self, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options)\u001b[0m\n\u001b[1;32m   1950\u001b[0m \u001b[39m\"\"\"Write dataset contents to a zarr group.\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m \n\u001b[1;32m   1952\u001b[0m \u001b[39mZarr chunks are determined in the following way:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[39m    The I/O user guide, with more details and examples.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_zarr\n\u001b[0;32m-> 2060\u001b[0m \u001b[39mreturn\u001b[39;00m to_zarr(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2061\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2062\u001b[0m     store\u001b[39m=\u001b[39;49mstore,\n\u001b[1;32m   2063\u001b[0m     chunk_store\u001b[39m=\u001b[39;49mchunk_store,\n\u001b[1;32m   2064\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2065\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   2066\u001b[0m     synchronizer\u001b[39m=\u001b[39;49msynchronizer,\n\u001b[1;32m   2067\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m   2068\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   2069\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[1;32m   2070\u001b[0m     consolidated\u001b[39m=\u001b[39;49mconsolidated,\n\u001b[1;32m   2071\u001b[0m     append_dim\u001b[39m=\u001b[39;49mappend_dim,\n\u001b[1;32m   2072\u001b[0m     region\u001b[39m=\u001b[39;49mregion,\n\u001b[1;32m   2073\u001b[0m     safe_chunks\u001b[39m=\u001b[39;49msafe_chunks,\n\u001b[1;32m   2074\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/xarray/backends/api.py:1638\u001b[0m, in \u001b[0;36mto_zarr\u001b[0;34m(dataset, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[39m# TODO: figure out how to properly handle unlimited_dims\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m dump_to_store(dataset, zstore, writer, encoding\u001b[39m=\u001b[39mencoding)\n\u001b[0;32m-> 1638\u001b[0m writes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49msync(compute\u001b[39m=\u001b[39;49mcompute)\n\u001b[1;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[1;32m   1641\u001b[0m     _finalize_store(writes, zstore)\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/xarray/backends/common.py:168\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[0;34m(self, compute)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# TODO: consider wrapping targets with dask.delayed, if this makes\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m# for any discernible difference in perforance, e.g.,\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m# targets = [dask.delayed(t) for t in self.targets]\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m delayed_store \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39;49mstore(\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msources,\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets,\n\u001b[1;32m    171\u001b[0m     lock\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock,\n\u001b[1;32m    172\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[1;32m    173\u001b[0m     flush\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     regions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregions,\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msources \u001b[39m=\u001b[39m []\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/dask/array/core.py:1229\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[39melif\u001b[39;00m compute:\n\u001b[1;32m   1228\u001b[0m     store_dsk \u001b[39m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1229\u001b[0m     compute_as_if_collection(Array, store_dsk, map_keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/dask/base.py:341\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"Compute a graph as if it were of type cls.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[1;32m    339\u001b[0m \u001b[39mAllows for applying the same optimizations and default scheduler.\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m schedule \u001b[39m=\u001b[39m get_scheduler(scheduler\u001b[39m=\u001b[39mscheduler, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, get\u001b[39m=\u001b[39mget)\n\u001b[0;32m--> 341\u001b[0m dsk2 \u001b[39m=\u001b[39m optimization_function(\u001b[39mcls\u001b[39;49m)(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/dask/array/optimization.py:66\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(dsk, keys, fuse_keys, fast_functions, inline_functions_fast_functions, rename_fused_keys, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     inline_functions_fast_functions \u001b[39m=\u001b[39m fast_functions\n\u001b[1;32m     64\u001b[0m hold \u001b[39m=\u001b[39m hold_keys(dsk, dependencies)\n\u001b[0;32m---> 66\u001b[0m dsk, dependencies \u001b[39m=\u001b[39m fuse(\n\u001b[1;32m     67\u001b[0m     dsk,\n\u001b[1;32m     68\u001b[0m     hold \u001b[39m+\u001b[39;49m keys \u001b[39m+\u001b[39;49m (fuse_keys \u001b[39mor\u001b[39;49;00m []),\n\u001b[1;32m     69\u001b[0m     dependencies,\n\u001b[1;32m     70\u001b[0m     rename_keys\u001b[39m=\u001b[39;49mrename_fused_keys,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m inline_functions_fast_functions:\n\u001b[1;32m     73\u001b[0m     dsk \u001b[39m=\u001b[39m inline_functions(\n\u001b[1;32m     74\u001b[0m         dsk,\n\u001b[1;32m     75\u001b[0m         keys,\n\u001b[1;32m     76\u001b[0m         dependencies\u001b[39m=\u001b[39mdependencies,\n\u001b[1;32m     77\u001b[0m         fast_functions\u001b[39m=\u001b[39minline_functions_fast_functions,\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/dask/optimization.py:867\u001b[0m, in \u001b[0;36mfuse\u001b[0;34m(dsk, keys, dependencies, ave_width, max_width, max_height, max_depth_new_edges, rename_keys, fuse_subgraphs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m key_renamer:\n\u001b[1;32m    866\u001b[0m     \u001b[39mfor\u001b[39;00m root_key, fused_keys \u001b[39min\u001b[39;00m fused_trees\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 867\u001b[0m         alias \u001b[39m=\u001b[39m key_renamer(fused_keys)\n\u001b[1;32m    868\u001b[0m         \u001b[39mif\u001b[39;00m alias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m alias \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m rv:\n\u001b[1;32m    869\u001b[0m             rv[alias] \u001b[39m=\u001b[39m rv[root_key]\n",
      "File \u001b[0;32m~/miniconda3/envs/geospatial/lib/python3.9/site-packages/dask/optimization.py:408\u001b[0m, in \u001b[0;36mdefault_fused_keys_renamer\u001b[0;34m(keys, max_fused_key_length)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m\"\"\"Create new keys for ``fuse`` tasks.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[39mThe optional parameter `max_fused_key_length` is used to limit the maximum string length for each renamed key.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39mIf this parameter is set to `None`, there is no limit.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m it \u001b[39m=\u001b[39m \u001b[39mreversed\u001b[39m(keys)\n\u001b[0;32m--> 408\u001b[0m first_key \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(it)\n\u001b[1;32m    409\u001b[0m typ \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(first_key)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m max_fused_key_length:  \u001b[39m# Take into account size of hash suffix\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fapar_maked_ds.to_zarr(out_dir+\"data/processed_data/noaa_nc/lai_fapar/filtered/fapar_filtered.zarr\")\n",
    "# ndvi_filtered.to_netcdf(out_dir+\"data/processed_data/noaa_nc/ndvi/filtered/ndvi_filtered.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read the LAI, NDVI and reflectance data and slect those NIR pixels we have good NDVI for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lai = xr.open_dataarray(dir+\"data/processed_data/noaa_nc/lai_fapar/filtered/lai_filtered.nc\")\n",
    "ndvi = xr.open_dataarray(dir+\"data/processed_data/noaa_nc/ndvi/filtered/ndvi_filtered.nc\")\n",
    "ds_refl = xr.open_zarr(dir+'data/raw_data/noaa_cdr/reflectance/noaa_reflectance_clipped_raw.zarr')\n",
    "nir = ds_refl[\"SREFL_CH2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_good = my_funs.isfinite(ndvi)\n",
    "nir_good = nir.where(ndvi_good)\n",
    "nir_good.to_netcdf(dir+\"data/processed_data/noaa_nc/reflectance/filtered/NIR_filtered.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the NIRv (ndvi*nir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_filtered = xr.open_dataarray(out_dir+\"data/processed_data/noaa_nc/lai_fapar/filtered/lai_filtered.nc\")\n",
    "lai_growing = growing_season(lai_filtered)\n",
    "lai_growing.to_netcdf(out_dir+\"data/processed_data/noaa_nc/lai_fapar/grouped/lai_growing.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xarray_Linear_trend(xarr, var_unit):\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    # getting shapes\n",
    "\n",
    "    m = np.prod(xarr.shape[1:]).squeeze()\n",
    "    n = xarr.shape[0]\n",
    "\n",
    "    # creating x and y variables for linear regression\n",
    "    x = xarr.time.to_pandas().index.to_julian_date().values[:, None]\n",
    "    y = xarr.to_masked_array().reshape(n, -1)\n",
    "\n",
    "    # ############################ #\n",
    "    # LINEAR REGRESSION DONE BELOW #\n",
    "    xm = x.mean(0)  # mean\n",
    "    ym = y.mean(0)  # mean\n",
    "    ya = y - ym  # anomaly\n",
    "    xa = x - xm  # anomaly\n",
    "\n",
    "    # variance and covariances\n",
    "    xss = (xa**2).sum(0) / (n - 1)  # variance of x (with df as n-1)\n",
    "    yss = (ya**2).sum(0) / (n - 1)  # variance of y (with df as n-1)\n",
    "    xys = (xa * ya).sum(0) / (n - 1)  # covariance (with df as n-1)\n",
    "    # slope and intercept\n",
    "    slope = xys / xss\n",
    "    intercept = ym - (slope * xm)\n",
    "    # statistics about fit\n",
    "    df = n - 2\n",
    "    r = xys / (xss * yss)**0.5\n",
    "    t = r * (df / ((1 - r) * (1 + r)))**0.5\n",
    "    p = stats.distributions.t.sf(abs(t), df)\n",
    "\n",
    "    # misclaneous additional functions\n",
    "    # yhat = dot(x, slope[None]) + intercept\n",
    "    # sse = ((yhat - y)**2).sum(0) / (n - 2)  # n-2 is df\n",
    "    # se = ((1 - r**2) * yss / xss / df)**0.5\n",
    "\n",
    "    # preparing outputs\n",
    "    out = xarr[:2].mean('time')\n",
    "    # first create variable for slope and adjust meta\n",
    "    xarr_slope = out.copy()\n",
    "    xarr_slope.name = '_slope'\n",
    "    xarr_slope.attrs['units'] = var_unit\n",
    "    xarr_slope.values = slope.reshape(xarr.shape[1:])\n",
    "    # do the same for the p value\n",
    "    xarr_p = out.copy()\n",
    "    xarr_p.name = '_Pvalue'\n",
    "    xarr_p.attrs[\n",
    "        'info'] = \"If p < 0.05 then the results from 'slope' are significant.\"\n",
    "    xarr_p.values = p.reshape(xarr.shape[1:])\n",
    "    # join these variables\n",
    "    xarr_out = xarr_slope.to_dataset(name='slope')\n",
    "    xarr_out['pval'] = xarr_p\n",
    "\n",
    "    return xarr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_growing = lai_growing.rename({\"year\":\"time\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = lai_growing.time.to_pandas().index\n",
    "b = pd.to_datetime(a, format='%Y')\n",
    "# lai_growing.time = b\n",
    "lai_growing[\"time\"] = b\n",
    "lai_growing_trend = xarray_trend(lai_growing, var_unit=\"m / m / year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "lai_growing_trend[\"slope\"].plot()\n",
    "plt.savefig(out_dir+\"/Figures/LAI_growing_trend.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_growing_mean = lai_growing.mean([\"latitude\",\"longitude\"])\n",
    "lai_growing_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('geospatial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58d4357dc51a73699c5449b037ed9e8ebf9460004e4993ef846dc1036514c650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
