{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pylab as plt\n",
    "import Rbeast as rb\n",
    "import numpy as np\n",
    "\n",
    "# import cartopy.crs as ccrss\n",
    "# import cartopy.feature as cf\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import my_funs\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.neighbors import BallTree, DistanceMetric\n",
    "\n",
    "# from causalimpact import CausalImpact\n",
    "import pickle\n",
    "from sklearn.impute import IterativeImputer\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "mpl.rcParams[\"mathtext.default\"] = \"regular\"\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from my_funs import outliers_index\n",
    "\n",
    "\n",
    "def year_detect(x):\n",
    "    # Get the index of the first non-nan value\n",
    "    if not np.any(x):\n",
    "        return np.nan\n",
    "    idx = np.argmax(x)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def most_common_values(arr, n):\n",
    "    # Count the n most repeated values in an array\n",
    "    counter = Counter(arr)\n",
    "    most_common = counter.most_common(n)\n",
    "    return [value for value, _ in most_common]\n",
    "\n",
    "\n",
    "dir = \"/data/home/hamiddashti/hamid/nasa_above/greeness/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = (\n",
    "    xr.open_dataset(\"../data/percent_cover.nc\")[\"__xarray_dataarray_variable__\"].sel(\n",
    "        time=slice(1984, 2013)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "ndvi = (\n",
    "    xr.open_dataarray(\"../data/NDVI_resampled_v2.nc\").sel(time=slice(\"1984\", \"2013\"))\n",
    "    / 10000\n",
    ")\n",
    "ndvi = ndvi.drop(\"band\")\n",
    "# lai = xr.open_dataarray(\"../data/lai4g_annual_max.nc\").sel(time=slice(\"1984\", \"2013\"))\n",
    "percent[\"lat\"] = ndvi[\"lat\"]\n",
    "percent[\"lon\"] = ndvi[\"lon\"]\n",
    "# Only focus on vegetative classes\n",
    "lc = percent.isel(band=[0, 2, 3, 4])  # EF, srub, herb and barren\n",
    "t_short = pd.date_range(start=\"1984\", end=\"2014\", freq=\"A-Dec\").year\n",
    "t = pd.date_range(start=\"1984\", end=\"2014\", freq=\"A-Dec\").year\n",
    "arr = xr.open_dataarray(\"../data/arr_id.nc\")\n",
    "# fires_1994 = gpd.read_file(\n",
    "#     \"../../data/shp_files/nbac_1986_to_2021_20220624/fire_1994_dissolved.shp\"\n",
    "# )\n",
    "# fires_1994 = fires[fires[\"YEAR\"] == 1994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires = gpd.read_file(\n",
    "    \"../../data/shp_files/nbac_1986_to_2021_20220624/nbac_1986_to_2021_20220624.shp\"\n",
    ")\n",
    "fires_1994 = fires[fires[\"YEAR\"] == 1994]\n",
    "fires_1994_crs = fires_1994.crs\n",
    "target_crs = CRS.from_epsg(4326)\n",
    "fires_1994_reprojected = fires_1994.to_crs(target_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_1994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distrubance and vegetation dynamic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCC map where at least one of the land covers changed more than 10% compared to the previous year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_diff = lc.diff(\"time\")\n",
    "net_change = lc_diff.where(lc_diff > 0.0).sum(\"band\")\n",
    "changed = (net_change >= 10).any(dim=\"time\")\n",
    "not_changed = (net_change < 10).all(dim=\"time\")\n",
    "mask_tmp = np.isfinite(lc).all([\"band\", \"time\"])\n",
    "not_changed = not_changed.where(mask_tmp == True)\n",
    "lc_changed = lc.where(changed == True)\n",
    "lc_not_changed = lc.where(not_changed == True)\n",
    "lc_changed_df = lc_changed.mean([\"lat\", \"lon\"]).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Rbeast time series model to each pixel's percent cover from 1984 to 2013 to get the non-linear trend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_changed = lc_changed.isel(band=0)\n",
    "sh_changed = lc_changed.isel(band=1)\n",
    "hb_changed = lc_changed.isel(band=2)\n",
    "sparse_changed = lc_changed.isel(band=3)\n",
    "\n",
    "metadata = rb.args(whichDimIsTime=1, season=\"none\", startTime=1984)\n",
    "prior = rb.args(trendMinSepDist=1)\n",
    "mcmc = rb.args(seed=1)\n",
    "extra = rb.args(  # a set of options to specify the outputs or computational configurations\n",
    "    dumpInputData=True,  # make a copy of the aggregated input data in the beast ouput\n",
    "    numThreadsPerCPU=1,  # Paralell  computing: use 2 threads per cpu core\n",
    "    numParThreads=0,  # `0` means using all CPU cores: total num of ParThreads = numThreadsPerCPU * core Num\n",
    "    printOptions=False,\n",
    "    computeTrendSlope=True,\n",
    "    computeCredible=True,\n",
    ")\n",
    "ef_changed_rbeast = rb.beast123(ef_changed.values, metadata, prior, mcmc, extra)\n",
    "sh_changed_rbeast = rb.beast123(sh_changed.values, metadata, prior, mcmc, extra)\n",
    "hb_changed_rbeast = rb.beast123(hb_changed.values, metadata, prior, mcmc, extra)\n",
    "sparse_changed_rbeast = rb.beast123(sparse_changed.values, metadata, prior, mcmc, extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "with open(\"../manuscript/outputs/ef_changed_rbeast\", \"wb\") as fp:\n",
    "    pickle.dump(ef_changed_rbeast, fp)\n",
    "with open(\"../manuscript/outputs/sh_changed_rbeast\", \"wb\") as fp:\n",
    "    pickle.dump(sh_changed_rbeast, fp)\n",
    "with open(\"../manuscript/outputs/hb_changed_rbeast\", \"wb\") as fp:\n",
    "    pickle.dump(hb_changed_rbeast, fp)\n",
    "with open(\"../manuscript/outputs/sparse_changed_rbeast\", \"wb\") as fp:\n",
    "    pickle.dump(sparse_changed_rbeast, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the trend and some other information from fitted models for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ef = ef_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_ef_mean = pd.DataFrame(Y_ef, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_ef_std = pd.DataFrame(Y_ef, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_ef = (\n",
    "    ef_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_ef = (\n",
    "    ef_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_ef = (\n",
    "    ef_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_ef = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_ef)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_ef_0 = ef_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_ef_1 = ef_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_ef_0_pd = ndvi.copy(data=ci_ef_0).mean([\"lat\", \"lon\"])\n",
    "ci_ef_1_pd = ndvi.copy(data=ci_ef_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_ef = most_common_values(cp_ef.values.squeeze(), 2)\n",
    "\n",
    "Y_sh = sh_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sh_mean = pd.DataFrame(Y_sh, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_sh_std = pd.DataFrame(Y_sh, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_sh = (\n",
    "    sh_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_sh = (\n",
    "    sh_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_sh = (\n",
    "    sh_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_sh = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_sh)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_sh_0 = sh_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sh_1 = sh_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sh_0_pd = ndvi.copy(data=ci_sh_0).mean([\"lat\", \"lon\"])\n",
    "ci_sh_1_pd = ndvi.copy(data=ci_sh_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_sh = most_common_values(cp_sh.values.squeeze(), 2)\n",
    "\n",
    "Y_hb = hb_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_hb_mean = pd.DataFrame(Y_hb, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_hb_std = pd.DataFrame(Y_hb, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_hb = (\n",
    "    hb_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_hb = (\n",
    "    hb_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_hb = (\n",
    "    hb_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_hb = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_hb)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_hb_0 = hb_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_hb_1 = hb_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_hb_0_pd = ndvi.copy(data=ci_hb_0).mean(\n",
    "    [\"lat\", \"lon\"]\n",
    ")  # Lets increse the size of the figured.cp  # Detected change points with probability more than 50%\n",
    "ci_hb_1_pd = ndvi.copy(data=ci_hb_1).mean([\"lat\", \"lon\"])\n",
    "\n",
    "Y_sparse = sparse_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sparse_mean = pd.DataFrame(Y_sparse, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_sparse_std = pd.DataFrame(Y_sparse, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_sparse = (\n",
    "    sparse_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_sparse = (\n",
    "    sparse_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_sparse = (\n",
    "    sparse_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_sparse = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_sparse)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_sparse_0 = sparse_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sparse_1 = sparse_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sparse_0_pd = ndvi.copy(data=ci_sparse_0).mean([\"lat\", \"lon\"])\n",
    "ci_sparse_1_pd = ndvi.copy(data=ci_sparse_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_sparse = most_common_values(cp_sparse.values.squeeze(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the NDVI of disturbed regions and fit the Rbeast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_changed = ndvi.where(changed == True)\n",
    "ndvi_not_changed = ndvi.where(not_changed == True)\n",
    "I = np.isfinite(ndvi_changed).sum(dim=\"time\") > 25\n",
    "ndvi_changed_ok = ndvi_changed.where(I == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = rb.args(whichDimIsTime=1, season=\"none\", startTime=1984)\n",
    "prior = rb.args(trendMinSepDist=1)\n",
    "mcmc = rb.args(seed=1)\n",
    "extra = rb.args(  # a set of options to specify the outputs or computational configurations\n",
    "    dumpInputData=True,  # make a copy of the aggregated input data in the beast ouput\n",
    "    numThreadsPerCPU=1,  # Paralell  computing: use 2 threads per cpu core\n",
    "    numParThreads=0,  # `0` means using all CPU cores: total num of ParThreads = numThreadsPerCPU * core Num\n",
    "    printOptions=False,\n",
    "    computeTrendSlope=True,\n",
    "    computeCredible=True,\n",
    ")\n",
    "ndvi_changed_rbeast = rb.beast123(ndvi_changed_ok.values, metadata, prior, mcmc, extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract some information from the fitted ndvi model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ndvi = ndvi_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "y_ndvi_mean = pd.DataFrame(Y_ndvi, index=t).dropna(axis=1).mean(axis=1)\n",
    "y_ndvi_std = pd.DataFrame(Y_ndvi, index=t).dropna(axis=1).std(axis=1)\n",
    "\n",
    "slp_ndvi = ndvi_changed_rbeast.trend.slp  # Vector of estimate slopes at each t\n",
    "cpOccPr_ndvi = (\n",
    "    ndvi_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "slpSgnPosPr_ndvi = ndvi_changed_rbeast.trend.slpSgnPosPr  # Sign of the slope\n",
    "cpPr_ndvi = (\n",
    "    ndvi_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "# cp_ndvi = ndvi_changed_rbeast.trend.cp[\n",
    "#     cpPr_ndvi > 0.5]  # Detected change points with probability more than 50%\n",
    "cp_ndvi = (\n",
    "    ndvi_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "# cpPr_ndvi = cpPr_ndvi[cpPr_ndvi > 0.5]\n",
    "ci_ndvi_0 = ndvi_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_ndvi_1 = ndvi_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "\n",
    "cp_ndvi = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_ndvi)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "slp_pos_mean = (\n",
    "    ndvi.copy(data=ndvi_changed_rbeast.trend.slpSgnPosPr).mean([\"lat\", \"lon\"]).values\n",
    ")\n",
    "slp_pos_std = (\n",
    "    ndvi.copy(data=ndvi_changed_rbeast.trend.slpSgnPosPr).std([\"lat\", \"lon\"]).values\n",
    ")\n",
    "slp_zero_mean = (\n",
    "    ndvi.copy(data=ndvi_changed_rbeast.trend.slpSgnZeroPr).mean([\"lat\", \"lon\"]).values\n",
    ")\n",
    "slp_neg_mean = 1 - (slp_pos_mean + slp_zero_mean)\n",
    "\n",
    "slp_ndvi_sd = (\n",
    "    ndvi.copy(data=ndvi_changed_rbeast.trend.slpSD).mean([\"lat\", \"lon\"]).values\n",
    ")\n",
    "most_common_years = most_common_values(cp_ndvi.values.squeeze(), 2)\n",
    "slp_ndvi_pd = ndvi.copy(data=slp_ndvi).mean([\"lat\", \"lon\"])\n",
    "ci_ndvi_0_pd = ndvi.copy(data=ci_ndvi_0).mean([\"lat\", \"lon\"])\n",
    "ci_ndvi_1_pd = ndvi.copy(data=ci_ndvi_1).mean([\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_changed_mean_rbeast = rb.beast(\n",
    "    ndvi_changed_ok_mean, season=\"none\", start=1984, tseg_minlength=1, ci=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ndvi_mean = ndvi_changed_mean_rbeast.trend.Y\n",
    "ci_ndvi_0_pd = ndvi_changed_mean_rbeast.trend.CI[:, 0]\n",
    "ci_ndvi_1_pd = ndvi_changed_mean_rbeast.trend.CI[:, 1]\n",
    "cp_ndvi = ndvi_changed_mean_rbeast.trend.cp\n",
    "most_common_years = most_common_values(cp_ndvi, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    3, 1, figsize=(6, 8), gridspec_kw={\"wspace\": 0.1, \"hspace\": 0.05}\n",
    ")\n",
    "ax[0].plot(t, y_ndvi_mean, \"-*\", color=\"green\", alpha=0.7, label=\"EF\")\n",
    "ax[0].fill_between(t, ci_ndvi_0_pd, ci_ndvi_1_pd, color=\"green\", alpha=0.2)\n",
    "# ax[0].set_ylabel(\"NDVI\", fontsize=14)\n",
    "ax[0].tick_params(axis=\"y\", colors=\"green\")\n",
    "ax[0].tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].xaxis.set_tick_params(width=0)\n",
    "ax[0].set_ylim(np.min(y_ndvi_mean) - 0.003, np.max(y_ndvi_mean) + 0.003)\n",
    "ax[0].set_xlim(1984, 2014)\n",
    "ax[0].vlines(\n",
    "    most_common_years,\n",
    "    ax[0].get_ylim()[0],\n",
    "    ax[0].get_ylim()[1],\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    3, 1, figsize=(6, 8), gridspec_kw={\"wspace\": 0.1, \"hspace\": 0.05}\n",
    ")\n",
    "ax[0].plot(t, y_ndvi_mean, \"-*\", color=\"green\", alpha=0.7, label=\"EF\")\n",
    "ax[0].fill_between(t, ci_ndvi_0_pd, ci_ndvi_1_pd, color=\"green\", alpha=0.2)\n",
    "# ax[0].set_ylabel(\"NDVI\", fontsize=14)\n",
    "ax[0].tick_params(axis=\"y\", colors=\"green\")\n",
    "ax[0].tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].xaxis.set_tick_params(width=0)\n",
    "ax[0].set_ylim(np.min(y_ndvi_mean) - 0.03, np.max(y_ndvi_mean) + 0.03)\n",
    "ax[0].set_xlim(1984, 2014)\n",
    "ax[0].vlines(\n",
    "    most_common_years,\n",
    "    ax[0].get_ylim()[0],\n",
    "    ax[0].get_ylim()[1],\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax[1].plot(t, slp_ndvi_pd, color=\"green\", linewidth=2, label=\"slope\")\n",
    "ax[1].fill_between(\n",
    "    t, slp_ndvi_pd - slp_ndvi_sd, slp_ndvi_pd + slp_ndvi_sd, color=\"green\", alpha=0.2\n",
    ")\n",
    "ax[1].set_ylim(-0.03, 0.02)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_xlim(1984, 2014)\n",
    "# ax[1].set_ylabel(\"Slope\", fontsize=14)ndvi\n",
    "ax[1].tick_params(axis=\"y\", colors=\"green\")\n",
    "ax[1].vlines(\n",
    "    most_common_years,\n",
    "    ax[1].get_ylim()[0],\n",
    "    ax[1].get_ylim()[1],\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[1].axhline(0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "ax[1].tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "ax02 = ax[1].twinx()\n",
    "ax02.fill_between(t, 0, slp_pos_mean, color=\"red\", alpha=0.5)\n",
    "ax02.set_ylim(0.15, 1)\n",
    "ax02.tick_params(axis=\"y\", colors=\"red\", labelsize=12)\n",
    "# ax02.set_ylabel(\"Prob of positive\\nslope\", fontsize=14)\n",
    "\n",
    "\n",
    "bin_centers = t[:-1] + np.diff(t) / 2\n",
    "bin_edges = np.concatenate(\n",
    "    ([t[0] - (t[1] - t[0]) / 2], bin_centers, [t[-1] + (t[1] - t[0]) / 2])\n",
    ")\n",
    "ax[2].hist(cp_ndvi, bins=bin_edges, color=\"black\", alpha=0.5)\n",
    "# ax[2].set_ylabel(\"Frequency of\\ndetected CPs\", fontsize=14, color=\"black\")\n",
    "ax[2].set_xlim(1984, 2014)\n",
    "ax[2].vlines(\n",
    "    most_common_years,\n",
    "    ax[2].get_ylim()[0],\n",
    "    ax[2].get_ylim()[1],\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[2].tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "for a in ax:\n",
    "    for label in a.get_xticklabels():\n",
    "        label.set_fontproperties(font_manager.FontProperties(weight=\"bold\"))\n",
    "    for label in a.get_yticklabels():\n",
    "        label.set_fontproperties(font_manager.FontProperties(weight=\"bold\"))\n",
    "\n",
    "for a in [ax02]:\n",
    "    for label in a.get_xticklabels():\n",
    "        label.set_fontproperties(font_manager.FontProperties(weight=\"bold\"))\n",
    "    for label in a.get_yticklabels():\n",
    "        label.set_fontproperties(font_manager.FontProperties(weight=\"bold\"))\n",
    "\n",
    "\n",
    "fig.align_ylabels()\n",
    "# plt.savefig(\n",
    "#     \"../manuscript/figures/NDVI_dynamics.png\", bbox_inches=\"tight\", pad_inches=0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference\n",
    "\n",
    "### Our goal is to find the impact of 1994 land cover change on NDVI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find years where rbeast found a CP in EF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_changed_cp = ndvi.isel(time=0).copy(data=ef_changed_rbeast.trend.cp[0, :, :])\n",
    "ef_changed_cpPr = ndvi.isel(time=0).copy(data=ef_changed_rbeast.trend.cpPr[0, :, :])\n",
    "ef_changed_cp_sig = ef_changed_cp.where(ef_changed_cpPr > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find those pixels where CPs happened in 1994, then take the spatial mean of percent covers and NDVI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"EF\", \"Shrub\", \"Herb\", \"Sparse\"]\n",
    "changed_1994 = ef_changed_cp_sig == 1994\n",
    "lc_changed_1994 = lc.where(changed_1994 == True)\n",
    "lc_changed_1994_df = lc_changed_1994.mean([\"lat\", \"lon\"]).to_pandas()\n",
    "lc_changed_1994_df.columns = names\n",
    "ef_changed_1994 = lc_changed_1994.isel(band=0)\n",
    "sh_changed_1994 = lc_changed_1994.isel(band=1)\n",
    "sparse_changed_1994 = lc_changed_1994.isel(band=3)\n",
    "\n",
    "ef_changed_1994 = lc_changed_1994.isel(band=0)\n",
    "ef_changed_1994_mean = ef_changed_1994.mean([\"lat\", \"lon\"])\n",
    "\n",
    "sh_changed_1994 = lc_changed_1994.isel(band=1)\n",
    "sh_changed_1994_mean = sh_changed_1994.mean([\"lat\", \"lon\"])\n",
    "\n",
    "hb_changed_1994 = lc_changed_1994.isel(band=2)\n",
    "hb_changed_1994_mean = hb_changed_1994.mean([\"lat\", \"lon\"])\n",
    "\n",
    "sparse_changed_1994 = lc_changed_1994.isel(band=3)\n",
    "sparse_changed_1994_mean = sparse_changed_1994.mean([\"lat\", \"lon\"])\n",
    "\n",
    "ndvi_changed_1994 = ndvi.where(changed_1994 == True)\n",
    "lc_not_changed_1994 = lc.where(changed_1994 == False)\n",
    "lc_not_changed_1994_stacked = lc_not_changed_1994.stack(z=[\"lon\", \"lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = rb.args(whichDimIsTime=1, season=\"none\", startTime=1984)\n",
    "prior = rb.args(trendMinSepDist=1)\n",
    "mcmc = rb.args(seed=1)\n",
    "extra = rb.args(  # a set of options to specify the outputs or computational configurations\n",
    "    dumpInputData=True,  # make a copy of the aggregated input data in the beast ouput\n",
    "    numThreadsPerCPU=1,  # Paralell  computing: use 2 threads per cpu core\n",
    "    numParThreads=0,  # `0` means using all CPU cores: total num of ParThreads = numThreadsPerCPU * core Num\n",
    "    printOptions=False,\n",
    "    computeTrendSlope=True,\n",
    "    computeCredible=True,\n",
    ")\n",
    "ef_changed_1994_rbeast = rb.beast123(\n",
    "    ef_changed_1994.values, metadata, prior, mcmc, extra\n",
    ")\n",
    "sh_changed_1994_rbeast = rb.beast123(\n",
    "    sh_changed_1994.values, metadata, prior, mcmc, extra\n",
    ")\n",
    "hb_changed_1994_rbeast = rb.beast123(\n",
    "    hb_changed_1994.values, metadata, prior, mcmc, extra\n",
    ")\n",
    "sparse_changed_1994_rbeast = rb.beast123(\n",
    "    sparse_changed_1994.values, metadata, prior, mcmc, extra\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ef_changed_1994 = ef_changed_1994_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_ef_changed_1994_mean = (\n",
    "    pd.DataFrame(Y_ef_changed_1994, index=t).dropna(axis=1).mean(axis=1)\n",
    ")\n",
    "ci_ef_0_1994 = ef_changed_1994_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_ef_1_1994 = ef_changed_1994_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_ef_0_1994_pd = ndvi.copy(data=ci_ef_0_1994).mean([\"lat\", \"lon\"])\n",
    "ci_ef_1_1994_pd = ndvi.copy(data=ci_ef_1_1994).mean([\"lat\", \"lon\"])\n",
    "\n",
    "Y_sh_changed_1994 = sh_changed_1994_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sh_changed_1994_mean = (\n",
    "    pd.DataFrame(Y_sh_changed_1994, index=t).dropna(axis=1).mean(axis=1)\n",
    ")\n",
    "ci_sh_0_1994 = sh_changed_1994_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sh_1_1994 = sh_changed_1994_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sh_0_1994_pd = ndvi.copy(data=ci_sh_0_1994).mean([\"lat\", \"lon\"])\n",
    "ci_sh_1_1994_pd = ndvi.copy(data=ci_sh_1_1994).mean([\"lat\", \"lon\"])\n",
    "\n",
    "Y_hb_changed_1994 = hb_changed_1994_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_hb_changed_1994_mean = (\n",
    "    pd.DataFrame(Y_hb_changed_1994, index=t).dropna(axis=1).mean(axis=1)\n",
    ")\n",
    "ci_hb_0_1994 = hb_changed_1994_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_hb_1_1994 = hb_changed_1994_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_hb_0_1994_pd = ndvi.copy(data=ci_hb_0_1994).mean([\"lat\", \"lon\"])\n",
    "ci_hb_1_1994_pd = ndvi.copy(data=ci_hb_1_1994).mean([\"lat\", \"lon\"])\n",
    "\n",
    "Y_sparse_changed_1994 = sparse_changed_1994_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sparse_changed_1994_mean = (\n",
    "    pd.DataFrame(Y_sparse_changed_1994, index=t).dropna(axis=1).mean(axis=1)\n",
    ")\n",
    "ci_sparse_0_1994 = sparse_changed_1994_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sparse_1_1994 = sparse_changed_1994_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sparse_0_1994_pd = ndvi.copy(data=ci_sparse_0_1994).mean([\"lat\", \"lon\"])\n",
    "ci_sparse_1_1994_pd = ndvi.copy(data=ci_sparse_1_1994).mean([\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset that conist of the obserbed NDVIs that changes in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of pixels within 50 km radius\n",
    "ndvi_changed_stacked = ndvi_changed_1994.stack(z=[\"lon\", \"lat\"])\n",
    "ndvi_not_changed_stacked = ndvi_not_changed.stack(z=[\"lon\", \"lat\"])\n",
    "\n",
    "# select a random year and calculate the distances within 50 km radius and get all indices\n",
    "changed_year_ndvi = ndvi_changed_stacked.isel(time=10)\n",
    "df = changed_year_ndvi.to_dataframe()\n",
    "coords = np.radians(df[[\"lat\", \"lon\"]])\n",
    "dist = DistanceMetric.get_metric(\"haversine\")\n",
    "tree = BallTree(coords, metric=dist)\n",
    "indices = tree.query_radius(coords, r=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_mean_neighbors = []\n",
    "data_ndvi = []\n",
    "arr_idx = []\n",
    "\n",
    "for k in tqdm(range(len(indices))):\n",
    "    # for k in tqdm(np.arange(457635, 457640)):\n",
    "    data = []\n",
    "    lc_tmp = []\n",
    "\n",
    "    idx = indices[k]\n",
    "    # Get the changed NDVI values of the centeral pixel\n",
    "    center_pixel = idx[np.where(idx == k)]\n",
    "    center_pixel_ndvi = ndvi_changed_stacked[:, center_pixel].values.squeeze()\n",
    "\n",
    "    # continue if the central pixel is nan\n",
    "    if np.isnan(center_pixel_ndvi).all():\n",
    "        continue\n",
    "    if np.isnan(center_pixel_ndvi).sum() > 5:\n",
    "        continue\n",
    "\n",
    "    data.append(center_pixel_ndvi)\n",
    "\n",
    "    # Go over the neighboring pixels and get the NDVI values of unchanged pixels\n",
    "    for i in range(len(idx)):\n",
    "        if idx[i] == center_pixel:\n",
    "            continue\n",
    "        tmp1 = ndvi_not_changed_stacked[:, idx[i]].values.squeeze()\n",
    "        # skip if the neighboring pixel is all nans\n",
    "        if np.isnan(tmp1).all():\n",
    "            continue\n",
    "        if np.isnan(tmp1).sum() > 5:\n",
    "            continue\n",
    "        data.append(tmp1)\n",
    "    data_ndvi.append(np.array(data).transpose())\n",
    "    arr_idx.append(k)  # Get the index of changed pixel\n",
    "    lc_mean_neighbors.append(lc_not_changed_1994_stacked[:, :, idx[i]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_ndvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../manuscript/outputs/disturbed_ndvi_1994\", \"wb\") as fp:\n",
    "    pickle.dump(data_ndvi, fp)\n",
    "with open(\"../manuscript/outputs/index_disturbed_ndvi_1994\", \"wb\") as fp:\n",
    "    pickle.dump(arr_idx, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the data for causal inference analyses is ready. We use R to do the analyses. Then import outputs from R to here for further analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "# Read the X and y values of NDVIs that are used in causal_impact.R2 file for fitting the rbeast and plotting it\n",
    "rdata = pyreadr.read_r(\"../manuscript/outputs/ci_results_data.RData\")\n",
    "y_ndvi = rdata[\"y_ndvi\"]\n",
    "X_ndvi = rdata[\"X_ndvi\"]\n",
    "data_mean = rdata[\"data_mean_ndvi\"]\n",
    "ndvi_changed_1994_mean_r = data_mean[\"y_mean_ndvi\"]\n",
    "ndvi_not_changed_1994_mean_r = data_mean[\"X_mean_ndvi\"]\n",
    "results_ndvi = rdata[\"results_ndvi\"]\n",
    "response_ndvi = rdata[\"response_ndvi\"].T\n",
    "pred_ndvi = rdata[\"pred_ndvi\"].T\n",
    "point_effect = rdata[\"point.effect_ndvi\"]\n",
    "point_effect_lower = rdata[\"point.effect.lower_ndvi\"].T\n",
    "point_effect_upper = rdata[\"point.effect.upper_ndvi\"].T\n",
    "pred_lower = rdata[\"pred.lower_ndvi\"].T\n",
    "pred_upper = rdata[\"pred.upper_ndvi\"].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the rbeast for the NDVI values used in causal inference analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = rb.args(whichDimIsTime=1, season=\"none\", startTime=1984)\n",
    "prior = rb.args(trendMinSepDist=1)\n",
    "mcmc = rb.args(seed=1)\n",
    "extra = rb.args(  # a set of options to specify the outputs or computational configurations\n",
    "    dumpInputData=True,  # make a copy of the aggregated input data in the beast ouput\n",
    "    numThreadsPerCPU=1,  # Paralell  computing: use 2 threads per cpu core\n",
    "    numParThreads=0,  # `0` means using all CPU cores: total num of ParThreads = numThreadsPerCPU * core Num\n",
    "    printOptions=False,\n",
    "    computeTrendSlope=True,\n",
    "    computeCredible=True,\n",
    ")\n",
    "ndvi_rbeast_X = rb.beast123(X_ndvi, metadata, prior, mcmc, extra)\n",
    "ndvi_rbeast_y = rb.beast123(y_ndvi, metadata, prior, mcmc, extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ndvi_mean_changed_1994 = ndvi_rbeast_y.trend.Y\n",
    "y_ndvi_mean_changed_1994_mean = pd.DataFrame(y_ndvi_mean_changed_1994, index=t).mean(\n",
    "    axis=1\n",
    ")\n",
    "ci_ndvi_0_pd_changed_1994 = ndvi_rbeast_y.trend.CI[:, 0]\n",
    "ci_ndvi_1_pd_changed_1994 = ndvi_rbeast_y.trend.CI[:, 1]\n",
    "ci_ndvi_0_pd_changed_1994 = pd.DataFrame(ci_ndvi_0_pd_changed_1994, index=t).mean(\n",
    "    axis=1\n",
    ")\n",
    "ci_ndvi_1_pd_changed_1994 = pd.DataFrame(ci_ndvi_1_pd_changed_1994, index=t).mean(\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "y_ndvi_mean_not_changed_1994 = ndvi_rbeast_X.trend.Y\n",
    "y_ndvi_mean_not_changed_1994_mean = pd.DataFrame(\n",
    "    y_ndvi_mean_not_changed_1994, index=t\n",
    ").mean(axis=1)\n",
    "ci_ndvi_0_pd_not_changed_1994 = ndvi_rbeast_X.trend.CI[:, 0]\n",
    "ci_ndvi_1_pd_not_changed_1994 = ndvi_rbeast_X.trend.CI[:, 1]\n",
    "ci_ndvi_0_pd_not_changed_1994 = pd.DataFrame(\n",
    "    ci_ndvi_0_pd_not_changed_1994, index=t\n",
    ").mean(axis=1)\n",
    "ci_ndvi_1_pd_not_changed_1994 = pd.DataFrame(\n",
    "    ci_ndvi_1_pd_not_changed_1994, index=t\n",
    ").mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "ci_variables = pyreadr.read_r(\"../manuscript/outputs/ci_aggregated.RData\")\n",
    "pred = ci_variables[\"pred\"].squeeze()\n",
    "pred_lower = ci_variables[\"pred.lower\"].squeeze()\n",
    "pred_upper = ci_variables[\"pred.upper\"].squeeze()\n",
    "response = ci_variables[\"response\"].squeeze()\n",
    "point_effect = ci_variables[\"point.effect\"].squeeze()\n",
    "point_effect_lower = ci_variables[\"point.effect.lower\"].squeeze()\n",
    "point_effect_upper = ci_variables[\"point.effect.upper\"].squeeze()\n",
    "\n",
    "# Calculate standard error of the mean\n",
    "y_mean_ndvi = np.nanmean(y_ndvi, axis=1)\n",
    "y_std_ndvi = np.nanstd(y_ndvi, axis=1)\n",
    "sem_y = y_std_ndvi / np.sqrt(y_ndvi.shape[0])\n",
    "y_upper_bound = y_mean_ndvi + sem_y\n",
    "y_lower_bound = y_mean_ndvi - sem_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything above ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ef = ef_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_ef_mean = pd.DataFrame(Y_ef, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_ef_std = pd.DataFrame(Y_ef, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_ef = (\n",
    "    ef_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_ef = (\n",
    "    ef_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_ef = (\n",
    "    ef_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_ef = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_ef)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_ef_0 = ef_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_ef_1 = ef_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_ef_0_pd = ndvi.copy(data=ci_ef_0).mean([\"lat\", \"lon\"])\n",
    "ci_ef_1_pd = ndvi.copy(data=ci_ef_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_ef = most_common_values(cp_ef.values.squeeze(), 2)\n",
    "\n",
    "Y_sh = sh_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sh_mean = pd.DataFrame(Y_sh, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_sh_std = pd.DataFrame(Y_sh, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_sh = (\n",
    "    sh_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_sh = (\n",
    "    sh_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_sh = (\n",
    "    sh_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_sh = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_sh)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_sh_0 = sh_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sh_1 = sh_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sh_0_pd = ndvi.copy(data=ci_sh_0).mean([\"lat\", \"lon\"])\n",
    "ci_sh_1_pd = ndvi.copy(data=ci_sh_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_sh = most_common_values(cp_sh.values.squeeze(), 2)\n",
    "\n",
    "Y_hb = hb_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_hb_mean = pd.DataFrame(Y_hb, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_hb_std = pd.DataFrame(Y_hb, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_hb = (\n",
    "    hb_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_hb = (\n",
    "    hb_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_hb = (\n",
    "    hb_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_hb = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_hb)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_hb_0 = hb_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_hb_1 = hb_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_hb_0_pd = ndvi.copy(data=ci_hb_0).mean(\n",
    "    [\"lat\", \"lon\"]\n",
    ")  # Lets increse the size of the figured.cp  # Detected change points with probability more than 50%\n",
    "ci_hb_1_pd = ndvi.copy(data=ci_hb_1).mean([\"lat\", \"lon\"])\n",
    "\n",
    "Y_sparse = sparse_changed_rbeast.trend.Y.reshape(30, -1)\n",
    "Y_sparse_mean = pd.DataFrame(Y_sparse, index=t).dropna(axis=1).mean(axis=1)\n",
    "Y_sparse_std = pd.DataFrame(Y_sparse, index=t).dropna(axis=1).std(axis=1)\n",
    "cpOccPr_sparse = (\n",
    "    sparse_changed_rbeast.trend.cpOccPr\n",
    ")  # Probability of occurance of a change point\n",
    "cpPr_sparse = (\n",
    "    sparse_changed_rbeast.trend.cpPr\n",
    ")  # Probability associated to a detected change point\n",
    "cp_sparse = (\n",
    "    sparse_changed_rbeast.trend.cp\n",
    ")  # Detected change points with probability more than 50%\n",
    "cp_sparse = pd.DataFrame(\n",
    "    ndvi.isel(time=range(9))\n",
    "    .copy(data=cp_sparse)\n",
    "    .isel(time=0)\n",
    "    .stack(z=[\"lat\", \"lon\"])\n",
    "    .values\n",
    ").dropna()\n",
    "ci_sparse_0 = sparse_changed_rbeast.trend.CI[:, 0, :, :]\n",
    "ci_sparse_1 = sparse_changed_rbeast.trend.CI[:, 1, :, :]\n",
    "ci_sparse_0_pd = ndvi.copy(data=ci_sparse_0).mean([\"lat\", \"lon\"])\n",
    "ci_sparse_1_pd = ndvi.copy(data=ci_sparse_1).mean([\"lat\", \"lon\"])\n",
    "most_common_cp_sparse = most_common_values(cp_sparse.values.squeeze(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (results_ndvi.iloc[:, 4] < 0.01).squeeze()\n",
    "sig_response_ndvi = response_ndvi[index]\n",
    "sig_pred_ndvi = pred_ndvi[index]\n",
    "sig_point_effect = point_effect[index]\n",
    "sig_point_effect_lower = point_effect_lower[index]\n",
    "sig_point_effect_upper = point_effect_upper[index]\n",
    "sig_pred_lower = pred_lower[index]\n",
    "sig_pred_upper = pred_upper[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard error of the mean\n",
    "sem_response = stats.sem(sig_response_ndvi, axis=0)\n",
    "\n",
    "# Calculate the means\n",
    "mean_response = sig_response_ndvi.mean(axis=0)\n",
    "mean_pred = sig_pred_ndvi.mean(axis=0)\n",
    "\n",
    "# Calculate the 95th percentile of the confidence interval\n",
    "upper_percentile = np.percentile(sig_pred_upper, 95, axis=0)\n",
    "lower_percentile = np.percentile(sig_pred_lower, 5, axis=0)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(8, 5),\n",
    "    gridspec_kw={\n",
    "        \"wspace\": 0.1,\n",
    "        \"hspace\": 0.05,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Plot observed data with SEM error bars\n",
    "ax.plot(t, mean_response, color=\"black\", label=\"Observed\")\n",
    "ax.errorbar(t, mean_response, yerr=sem_response, fmt=\"none\", color=\"gray\", alpha=0.5)\n",
    "\n",
    "# Plot predicted data with 95th percentile error bars\n",
    "ax.plot(t, mean_pred, color=\"red\", label=\"Predicted\")\n",
    "ax.errorbar(\n",
    "    t,\n",
    "    mean_pred,\n",
    "    yerr=[mean_pred - lower_percentile, upper_percentile - mean_pred],\n",
    "    fmt=\"none\",\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Add legend and labels\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculate standard error of the mean\n",
    "sem_response = stats.sem(sig_response_ndvi, axis=0)\n",
    "\n",
    "# Calculate the means\n",
    "mean_response = sig_response_ndvi.mean(axis=0)\n",
    "mean_pred = sig_pred_ndvi.mean(axis=0)\n",
    "\n",
    "# Calculate the 95th percentile of the confidence interval\n",
    "upper_percentile = np.percentile(sig_pred_upper, 95, axis=0)\n",
    "lower_percentile = np.percentile(sig_pred_lower, 5, axis=0)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(\n",
    "    2,\n",
    "    1,\n",
    "    figsize=(6, 8),\n",
    "    gridspec_kw={\n",
    "        \"wspace\": 0.1,\n",
    "        \"hspace\": 0.05,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Plot observed data with SEM filled in\n",
    "ax[0].plot(t, mean_response, color=\"black\", label=\"Observed\")\n",
    "ax[0].fill_between(\n",
    "    t,\n",
    "    mean_response - sem_response,\n",
    "    mean_response + sem_response,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Plot predicted data\n",
    "ax[0].plot(t, mean_pred, color=\"red\", label=\"Predicted\")\n",
    "ax[0].fill_between(t, lower_percentile, upper_percentile, color=\"red\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate standard error of the mean\n",
    "sem_response = stats.sem(sig_response_ndvi, axis=0)\n",
    "\n",
    "# Calculate the means\n",
    "mean_response = sig_response_ndvi.mean(axis=0)\n",
    "mean_pred = sig_pred_ndvi.mean(axis=0)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(\n",
    "    2,\n",
    "    1,\n",
    "    figsize=(6, 8),\n",
    "    gridspec_kw={\n",
    "        \"wspace\": 0.1,\n",
    "        \"hspace\": 0.05,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Plot observed data with SEM filled in\n",
    "ax[0].plot(t, mean_response, color=\"black\", label=\"Observed\")\n",
    "ax[0].fill_between(\n",
    "    t,\n",
    "    mean_response - sem_response,\n",
    "    mean_response + sem_response,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Plot predicted data\n",
    "ax[0].plot(t, mean_pred, color=\"red\", label=\"Predicted\")\n",
    "ax[0].fill_between(\n",
    "    t,\n",
    "    sig_pred_lower.mean(axis=0),\n",
    "    sig_point_effect_upper.mean(axis=0),\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_pred_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Rbeast to this mean rdata and the LCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_changed_1994_mean_rbeast = rb.beast(\n",
    "    ndvi_changed_1994_mean_r, season=\"none\", start=1984, tseg_minlength=1\n",
    ")\n",
    "ndvi_not_changed_1994_mean_rbeast = rb.beast(\n",
    "    ndvi_not_changed_1994_mean_r, season=\"none\", start=1984, tseg_minlength=1\n",
    ")\n",
    "\n",
    "ef_changed_1994_mean_rbeast = rb.beast(\n",
    "    ef_changed_1994_mean, season=\"none\", start=1984, tseg_minlength=1\n",
    ")\n",
    "sh_changed_1994_mean_rbeast = rb.beast(\n",
    "    sh_changed_1994_mean, season=\"none\", start=1984, tseg_minlength=1\n",
    ")\n",
    "hb_changed_1994_mean_rbeast = rb.beast(\n",
    "    hb_changed_1994_mean, season=\"none\", start=1984, tseg_minlength=1\n",
    ")\n",
    "sparse_changed_1994_mean_rbeast = rb.beast(\n",
    "    sparse_changed_1994_mean, season=\"none\", start=1984, tseg_minlength=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ndvi_changed_1994 = ndvi_changed_1994_mean_rbeast.trend.Y\n",
    "CI_ndvi_changed_1994 = ndvi_changed_1994_mean_rbeast.trend.CI\n",
    "\n",
    "Y_ndvi_not_changed_1994 = ndvi_not_changed_1994_mean_rbeast.trend.Y\n",
    "CI_ndvi_not_changed_1994 = ndvi_not_changed_1994_mean_rbeast.trend.CI\n",
    "ndvi_not_changed_1994_mean_rbeast.trend.slp\n",
    "\n",
    "Y_ef_changed_1994 = ef_changed_1994_mean_rbeast.trend.Y\n",
    "CI_ef_changed_1994 = ef_changed_1994_mean_rbeast.trend.CI\n",
    "\n",
    "Y_sh_changed_1994 = sh_changed_1994_mean_rbeast.trend.Y\n",
    "CI_sh_changed_1994 = sh_changed_1994_mean_rbeast.trend.CI\n",
    "\n",
    "Y_hb_changed_1994 = hb_changed_1994_mean_rbeast.trend.Y\n",
    "CI_hb_changed_1994 = hb_changed_1994_mean_rbeast.trend.CI\n",
    "\n",
    "Y_sparse_changed_1994 = sparse_changed_1994_mean_rbeast.trend.Y\n",
    "CI_sparse_changed_1994 = sparse_changed_1994_mean_rbeast.trend.CI"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
