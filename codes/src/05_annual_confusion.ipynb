{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script calculate the confusion table and associated LST, ET and albedo\n",
    "# and save it in a netcdf file\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from rasterio import features\n",
    "from rasterio.mask import mask\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def add_time_dim(xda):\n",
    "    xda = xda.expand_dims(time=[datetime.now()])\n",
    "    return xda\n",
    "\n",
    "\n",
    "def mymask(tif, shp):\n",
    "    # To mask landsat LUC pixels included in each MODIS pixel\n",
    "    out_image, out_transform = rasterio.mask.mask(\n",
    "        tif, shp, all_touched=False, crop=True\n",
    "    )\n",
    "    # out_meta = tif.meta\n",
    "    # return out_image,out_meta,out_transform\n",
    "    return out_image, out_transform\n",
    "\n",
    "\n",
    "def confusionmatrix(actual, predicted, unique, imap):\n",
    "    \"\"\"\n",
    "    Generate a confusion matrix for multiple classification\n",
    "    @params:\n",
    "        actual      - a list of integers or strings for known classes\n",
    "        predicted   - a list of integers or strings for predicted classes\n",
    "        # normalize   - optional boolean for matrix normalization\n",
    "        unique\t\t- is the unique numbers assigned to each class\n",
    "        imap\t\t- mapping of classes\n",
    "\n",
    "    @return:\n",
    "        matrix      - a 2-dimensional list of pairwise counts\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = [[0 for _ in unique] for _ in unique]\n",
    "    # Generate Confusion Matrix\n",
    "    for p, a in list(zip(actual, predicted)):\n",
    "        if (p > len(unique)) or (a > len(unique)):\n",
    "            continue\n",
    "        matrix[imap[p]][imap[a]] += 1\n",
    "    # Matrix Normalization\n",
    "    # if normalize:\n",
    "    sigma = sum([sum(matrix[imap[i]]) for i in unique])\n",
    "    matrix_normalized = [\n",
    "        row for row in map(lambda i: list(map(lambda j: j / sigma, i)), matrix)\n",
    "    ]\n",
    "    return matrix, matrix_normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUMBER_OF_CLASSES = 10  # [DF,DF,shrub,herb,sparse,wetland, water]\n",
    "class_names = [\n",
    "    \"EF\",\n",
    "    \"DF\",\n",
    "    \"Shrub\",\n",
    "    \"Herb\",\n",
    "    \"Sparse\",\n",
    "    \"Barren\",\n",
    "    \"Fen\",\n",
    "    \"Bog\",\n",
    "    \"SL\",\n",
    "    \"water\",\n",
    "]\n",
    "conversion_type = []\n",
    "for i in range(0, NUMBER_OF_CLASSES):\n",
    "    for j in range(0, NUMBER_OF_CLASSES):\n",
    "        # if (i==j):\n",
    "        # \tcontinue\n",
    "        tmp = class_names[i] + \"_\" + class_names[j]\n",
    "        conversion_type.append(tmp)\n",
    "dir = \"/data/home/hamiddashti/hamid/nasa_above/greeness/\"\n",
    "\n",
    "luc_dir = dir + \"data/raw_data/landcover/mosaic/\"\n",
    "out_dir = dir + \"data/processed_data/confusion_tables/\"\n",
    "\n",
    "changed_pixels_mask = xr.open_dataarray(\n",
    "    dir + \"data/processed_data/noaa_nc/lai_fapar/trend/changed_pixels.nc\"\n",
    ")\n",
    "shape_file = dir + \"data/shp_files/python_grid.shp\"\n",
    "\n",
    "with fiona.open(shape_file, \"r\") as shapefile:\n",
    "    shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    # area = [feature[\"properties\"][\"area\"] for feature in shapefile]\n",
    "# Calculate area\n",
    "shp_cart = gpd.read_file(shape_file)\n",
    "shp_cart = shp_cart.set_crs(4326)\n",
    "shp_cart = shp_cart.copy()\n",
    "shp_cart = shp_cart.to_crs({\"init\": \"epsg:3857\"})\n",
    "shp_cart.crs\n",
    "area = shp_cart[\"geometry\"].area / 10**6\n",
    "\n",
    "for year in np.arange(1984, 2013):\n",
    "    print(year + 1)\n",
    "\n",
    "    luc1 = rasterio.open(luc_dir + \"mosaic_reproject_\" + str(year) + \".tif\")\n",
    "    luc2 = rasterio.open(luc_dir + \"mosaic_reproject_\" + str(year + 1) + \".tif\")\n",
    "    changed_pixels_mask_val = np.ravel(changed_pixels_mask.values, order=\"F\")\n",
    "\n",
    "    pix_index = []\n",
    "    final_confusion = []\n",
    "    final_normal_confusion = []\n",
    "    final_area = []\n",
    "    final_percent_1 = []\n",
    "    final_percent_2 = []\n",
    "    final_dlcc = []\n",
    "\n",
    "    unique = np.arange(1, NUMBER_OF_CLASSES + 1)\n",
    "    imap = {key: i for i, key in enumerate(unique)}\n",
    "    for i in range(len(shapes)):\n",
    "        if changed_pixels_mask_val[i] == 0:\n",
    "            continue\n",
    "        luc1_masked = mymask(tif=luc1, shp=[shapes[i]])[0]\n",
    "        luc2_masked = mymask(tif=luc2, shp=[shapes[i]])[0]\n",
    "        try:\n",
    "            conf_tmp, conf_normal_tmp = np.asarray(\n",
    "                confusionmatrix(luc1_masked.ravel(), luc2_masked.ravel(), unique, imap)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            # This error mostly happens at the border of the study area,\n",
    "            # where after clipping it with shapefile only left values are\n",
    "            # 255 and 254 (i.e. nan values)\n",
    "            print(\"ZeroDivisionError\")\n",
    "            continue\n",
    "        count_1 = []\n",
    "        count_2 = []\n",
    "        for j in np.arange(1, 11):\n",
    "            count_1_tmp = (luc1_masked == j).sum()\n",
    "            count_1.append(count_1_tmp)\n",
    "            count_2_tmp = (luc2_masked == j).sum()\n",
    "            count_2.append(count_2_tmp)\n",
    "        percent_1 = count_1 / (np.sum(count_1))\n",
    "        percent_2 = count_2 / (np.sum(count_2))\n",
    "        dlcc_val = percent_2 - percent_1\n",
    "        # conf_tmp2 = np.ravel(conf_tmp, order=\"C\")\n",
    "        # conf_normal_tmp2 = np.ravel(conf_normal_tmp, order=\"C\")\n",
    "        final_confusion.append(conf_tmp)\n",
    "        final_normal_confusion.append(conf_normal_tmp)\n",
    "\n",
    "        pix_index.append(i)\n",
    "        final_area.append(area[i])\n",
    "        final_percent_1.append(percent_1)\n",
    "        final_percent_2.append(percent_2)\n",
    "        final_dlcc.append(dlcc_val)\n",
    "\n",
    "    pix_index = np.array(pix_index)\n",
    "    final_confusion = np.array(final_confusion)\n",
    "    final_normal_confusion = np.array(final_normal_confusion)\n",
    "\n",
    "    final_area = np.array(final_area)\n",
    "    final_percent_1 = np.array(final_percent_1)\n",
    "    final_percent_2 = np.array(final_percent_2)\n",
    "    final_dlcc = np.array(final_dlcc)\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"CONFUSION\": ((\"ID\", \"LC_t1\", \"LC_t2\"), final_confusion),\n",
    "            \"NORMALIZED_CONFUSION\": ((\"ID\", \"LC_t1\", \"LC_t2\"), final_normal_confusion),\n",
    "            \"DLCC\": ((\"ID\", \"LC\"), final_dlcc),\n",
    "            \"LC_2003\": ((\"ID\", \"LC\"), final_percent_1),\n",
    "            \"LC_2013\": ((\"ID\", \"LC\"), final_percent_2),\n",
    "            \"PIX_INDEX\": ((\"ID\"), pix_index),\n",
    "            \"Area\": ((\"ID\"), final_area),\n",
    "        },\n",
    "        coords={\n",
    "            \"ID\": range(len(final_area)),\n",
    "            \"LC_t1\": range(1, 11),\n",
    "            \"LC_t2\": range(1, 11),\n",
    "            \"LC\": range(1, 11),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds.to_netcdf(\n",
    "        (dir + \"/data/processed_data/confusion_tables/ct_corrected_\" + str(year + 1) + \".nc\")\n",
    "    )\n",
    "\n",
    "# Collect all dataset in one along time dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_mfdataset(\n",
    "    dir + \"/data/processed_data/confusion_tables/ct_corrected_*\", preprocess=add_time_dim\n",
    ")\n",
    "t = pd.date_range(start=\"1985\", end=\"2014\", periods=None, freq=\"A-DEC\")\n",
    "data[\"time\"] = t\n",
    "data.to_netcdf(dir + \"/data/processed_data/confusion_tables/ct_all_years_corrected.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('geospatial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58d4357dc51a73699c5449b037ed9e8ebf9460004e4993ef846dc1036514c650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
